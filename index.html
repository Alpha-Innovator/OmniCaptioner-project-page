<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="AdaptiveDiffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Dolphin</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <style>
    .sidebar {
        width: 200px;
        background-color: #f1f1f1;
        padding: 20px;
    }
    .sidebar ul {
        list-style: none;
    }
    .sidebar li {
        padding: 10px;
        margin: 5px 0;
        cursor: pointer;
        border-radius: 5px;
        transition: background-color 0.3s;
    }
    .sidebar li:hover {
        background-color: #ddd;
    }
    .sidebar li.active {
        background-color: #4CAF50;
        color: white;
    }
    .content {
        flex: 1;
        padding: 20px;
    }
    .content-section {
        display: none;
    }
    .content-section.active {
        display: block;
    }
  </style>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title", style="font-size: 2.5rem">
              <!-- <img src="static/images/apple-touch-icon.png" style="width: 40pt"> -->
              OmniCaptioner: One Captioner to Rule Them All!
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=pBFOTBwAAAAJ" target="_blank">Yiting Lu</a><sup>1,2,*</sup>,</span>
              <span class="author-block">
                <a href="https://jiakangyuan.github.io/" target="_blank">Jiakang Yuan</a><sup>2,3,*</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Zhen Li</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Shitian Zhao</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Qi Qin</a><sup>3</sup>,</span>
              <br>
                <span class="author-block">
                <a href="" target="_blank">Xinyue Li</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Le Zhuo</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Licheng Wen</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Dongyang Liu</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="http://leibai.site/" target="_blank">Lei Bai</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Yuewen Cao</a><sup>3</sup>,</span>
              <br>
              <span class="author-block">
                <a href="" target="_blank">Xin Li</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Botian Shi</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://eetchen.github.io/" target="_blank">Tao Chen</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="" target="_blank">Zhibo Chen</a><sup>1,ðŸ“§</sup>,</span>
              <span class="author-block">
                <a href="https://bobrown.github.io/boZhang.github.io/" target="_blank"> Bo Zhang</a><sup>3,ðŸ“§</sup>,</span>          
              <span class="author-block">
                <a href="" target="_blank">Peng Gao</a><sup>3</sup>,</span>
              </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <sup>1</sup>University of Science and Technology of China  
                  <sup>2</sup>Fudan University<br>
                  <sup>3</sup>Shanghai Artificial Intelligence Laboratory<br>
                </span>
                <span class="eql-cntrb" style="font-size:15pt"><br><sup>*</sup>Equal contribution,  <sup>ðŸ“§</sup>Corresponding author</span>
                <!-- <br>
                <span class="eql-cntrb">luyt31415@mail.ustc.edu.cn, jkyuan22@m.fudan.edu.cn,
                  chenzhibo@ustc.edu.cn, {zhangbo,gaopeng}@pjlab.org.cn</span> -->
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-download"></i>
                  </span>
                  <span>Dataset (Coming Soon)</span>
                </a>
              </span>

                <!-- ArXiv abstract Link -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container", style="max-width: 900px">
       <div class="item">
        <!-- <h2 class="subtitle has-text-centered", style="font-size: 1.5rem; font-weight: bolder">
          Different prompts may require different steps of noise prediction!!!
        </h2> -->
        <!-- Your image here -->
        <img src="static/images/Figure1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered", style="font-size: 1rem">
          OmniCaptioner: the top section demonstrates its capability to process diverse visual domains. The bottom section highlights itsapplications in visual reasoning (associated with reasoning LLM), image generation (integrated with T2I generation models), and efficient downstream SFT tasks adaptation.
        </h2>
      </div>
  </div>
</div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (<em>*e.g.</em>, natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (<em>*e.g.</em>, posters, UIs, textbooks), and structured visuals (<em>*e.g.</em>, documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper Method -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div style="width: 90%;">
        <h2 class="title is-3">OmniCaptioner</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.2rem; font-weight: bolder">
            To achieve a unified multimodal pretraining paradigm and handle diverse visual domains, we first construct a diverse caption dataset.
          </p>
          <p>
            <img src="static/images/data_pipeline.png" alt="MY ALT TEXT" style="margin-top: 20pt; width: 90%; margin-left: 5%;"/>
            <h2 class="subtitle has-text-centered">
              OmniCaptioner's diverse visual captioning pipeline. The pipeline consists of Seed-Caption Generation and Caption Extension. OmniCaptioner utilizes a 21M-caption dataset, covering diverse domains beyond natural images, enabling more comprehensive captioning capabilities.
            </h2> 
          </p>
          <p style="font-size: 1.2rem; font-weight: bolder">
            After a unified pretraining process, OmniCaptioner can effectively adapt to diverse downstream tasks including (i) Improved Visual Reasoning Tasks with LLMs, (ii) Enhanced Image Generation and Conversion, and (iii) Efficient SFT Process.
          </p>
          <p>
            <img src="static/images/data_pipeline.png" alt="MY ALT TEXT" style="margin-top: 20pt; width: 90%; margin-left: 5%;"/>
            <h2 class="subtitle has-text-centered", style="font-size: 1.2rem; font-weight: bolder">
              Illustration of OmniCaptioner's plug-and-play applications (Sub-figure a, b) and comparison between OmniCaptioner and LLava-OneVision-7B on non-natural image captioning (Sub-figure c). 
            </h2> 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Method -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="static/images/Dolphin_video_720.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen autoplay></iframe> -->
            <!-- <video width="640" height="360" autoplay mute loop controls>
              <source src="static/images/Dolphin_video_720.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->

<!-- Paper Experiment -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div style="width: 90%;">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            <img src="static/images/results.png" alt="MY ALT TEXT" style="margin-top: 20pt; width: 90%; margin-left: 5%;"/>
            <h2 class="subtitle has-text-centered", style="font-size: 1.2rem; font-weight: bolder">
              Results on three tasks including image classification, 3D classification, and sentiment classification.
            </h2> 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Experiment -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code> -->
        <!-- @article{ye2024training,
          title={Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy},
          author={Ye, Hancheng and Yuan, Jiakang and Xia, Renqiu and Yan, Xiangchao and Chen, Tao and Yan, Junchi and Shi, Botian and Zhang, Bo},
          journal={Advances in Neural Information Processing Systems},
          volume={36},
          year={2024}
        } -->
      <!-- </code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
